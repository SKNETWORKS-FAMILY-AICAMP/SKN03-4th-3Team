import streamlit as st
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from dotenv import load_dotenv
import openai
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# BLIP ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="êµí†µ í‘œì§€íŒ ì„¤ëª… ìƒì„±ê¸°", page_icon="ğŸš¦")

# ì±„íŒ… ê¸°ë¡ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
if "messages" not in st.session_state:
    st.session_state.messages = []

# ë©”ì¸ í˜ì´ì§€ íƒ€ì´í‹€ê³¼ ì„¤ëª…
st.title("ğŸš¦ êµí†µ í‘œì§€íŒ ì„¤ëª… ìƒì„±ê¸°")
st.subheader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  ì´ë¯¸ì§€ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”.")

# ì´ë¯¸ì§€ ì—…ë¡œë“œ ì˜ì—­
uploaded_file = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "jpeg", "png"])

# ì—…ë¡œë“œëœ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•˜ê³  í‘œì‹œ
if uploaded_file:
    image = Image.open(uploaded_file)
    if "uploaded_image" not in st.session_state:
        st.session_state.uploaded_image = image
        st.session_state.messages.append({"role": "user", "type": "image", "content": image})

# ì´ì „ ì±„íŒ… ë©”ì‹œì§€ì™€ ì´ë¯¸ì§€ë¥¼ í™”ë©´ì— í‘œì‹œ
for message in st.session_state.messages:
    if isinstance(message, dict) and "type" in message:
        if message["type"] == "text":
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        elif message["type"] == "image":
            with st.chat_message("user"):
                st.image(message["content"], caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_column_width=True)

# ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ ë°›ê¸°
if prompt := st.chat_input("ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    if uploaded_file is None:
        st.error("ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì„ ì±„íŒ…ì— ì¶”ê°€
        st.session_state.messages.append({"role": "user", "type": "text", "content": prompt})

        # BLIP ëª¨ë¸ì„ ì‚¬ìš©í•´ ê¸°ë³¸ ì„¤ëª… ìƒì„±
        inputs = processor(images=st.session_state.uploaded_image, return_tensors="pt")
        out = model.generate(**inputs)
        blip_description = processor.decode(out[0], skip_special_tokens=True)

        # GPT ëª¨ë¸ì„ ì‚¬ìš©í•´ BLIP ì„¤ëª…ì„ í™•ì¥í•˜ì—¬ ë³´ë‹¤ ì„¸ë°€í•œ ì„¤ëª… ìƒì„±
        gpt_prompt = (
            f"ë‹¤ìŒ ì´ë¯¸ì§€ëŠ” êµí†µ í‘œì§€íŒì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤: '{blip_description}'. "
            f"ì´ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë”ìš± êµ¬ì²´ì ì´ê³  ì„¸ë°€í•œ êµí†µ í‘œì§€íŒ ì„¤ëª…ì„ ìƒì„±í•˜ì„¸ìš”. "
            "íŠ¹íˆ í•´ë‹¹ í‘œì§€íŒì˜ ê¸°ëŠ¥, ìš´ì „ìì—ê²Œ ìš”êµ¬ë˜ëŠ” í–‰ë™, ê·¸ë¦¬ê³  ì£¼ì˜ ì‚¬í•­ì— ëŒ€í•´ í¬í•¨í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”."
        )

        # GPT-4 API í˜¸ì¶œ
        response = openai.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” êµí†µ í‘œì§€íŒì— ëŒ€í•œ ì„¤ëª…ì„ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": gpt_prompt}
            ]
        )
        gpt_description = response.choices[0].message.content.strip()

        # GPT ì„¤ëª…ì„ ì±„íŒ…ì— ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "type": "text", "content": gpt_description})

        # ê²°ê³¼ë¥¼ í™”ë©´ì— í‘œì‹œ (ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ)
        with st.chat_message("assistant"):
            st.markdown(gpt_description)
